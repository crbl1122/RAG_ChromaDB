# RAG App with ChromaDB + LlamaIndex + BGE embeddings + GPT-4o-mini

## Overview

This project implements a Retrieval-Augmented Generation (RAG) solution that allows users to ask questions about documents (e.g., Deutsche Telekom press releases) and get answers based on the most relevant text chunks.

The app ingests `.txt` documents from the `data/` directory, chunks them, creates embeddings using the BAAI BGE model, stores vectors and metadata in ChromaDB, and retrieves relevant chunks to provide context for a public LLM (`gpt-4o-mini`) to generate answers.

The UI is a Streamlit app for easy deployment and interaction.

---

## Features

- **Data Store:** Uses [ChromaDB](https://github.com/chroma-core/chroma) as a vector database.
- **Data Ingestion & Indexing:** Reads text files, chunks with `llama_index` SentenceSplitter, embeds with `BAAI/bge-small-en-v1.5`, stores in ChromaDB.
- **Retrieval:** Semantic search on the vector store to find top relevant chunks.
- **LLM Integration:** Uses OpenAI's GPT-4o-mini via API to answer questions based on retrieved context.
- **UI:** Streamlit app for querying and viewing answers along with source document chunks.
- **License Compliance:** All dependencies are open source with permissive licenses (MIT/Apache 2.0).
- **API Key Handling:** Requires user to provide their own OpenAI API key via environment variable.

---

## Setup & Usage

1. **Clone the repository**

```bash
gh repo clone crbl1122/RAG_ChromaDB
cd RAG_ChromaDB
```

2. **(Optional but recommended) Create and activate a virtual environment**

- On Linux/macOS:

```bash
python3 -m venv venv
source venv/bin/activate
```

- On Windows (PowerShell):

```powershell
python -m venv venv
.env\Scripts\Activate.ps1
```

3. **Install dependencies**

```bash
pip install -r requirements.txt
```

4. **Add your documents**

Place `.txt` files (e.g., Deutsche Telekom press releases) into the `data/` folder.

5. **Set your OpenAI API key**

```bash
export OPENAI_API_KEY="your_openai_api_key"
```

(On Windows PowerShell use: `setx OPENAI_API_KEY "your_openai_api_key"` and restart terminal)

6. **Run the app**

```bash
streamlit run app.py
```

7. **Use the UI**

- Enter a question in the input box.
- View the answer generated by GPT-4o-mini.
- Expand "Show Retrieved Context" to see document chunks used as context.

---

## Summary

| Requirement                      | Status               |
|---------------------------------|----------------------|
| Data store                      | ✅ ChromaDB vector DB |
| Data ingestion/indexing         | ✅ Chunking + embedding + indexing in ChromaDB |
| Retrieval component             | ✅ Semantic search via ChromaDB |
| Public LLM connection           | ✅ OpenAI GPT-4o-mini |
| Interactive UI                  | ✅ Streamlit app with Q&A input |
| Singular question (not chatbot) | ✅ Yes                |
| Open source frameworks (MIT/Apache) | ✅ All dependencies comply |
| API keys handled properly       | ✅ User sets environment variable |
| Deployable & simple             | ✅ Streamlit is easy to deploy |

---

## Notes

- Replace the contents of the `data/` folder with your own documents to adapt to other domains.
- ChromaDB persists the index in the `chromadb_store` folder.
- Ensure you have a compatible version of Python (>=3.8).

---

## License

This project uses open source libraries under MIT and Apache 2.0 licenses.

---

## Contact

For questions or issues, please open an issue in the repository or contact the maintainer.
