# Measures to Improve Retrieval in RAG:

## Query Expansion with Generated Answers:

- **Enhance Queries with LLM-generated Answers**: Augment the original user query with answers generated by the LLM. This new query, combining both the original query and the generated answer, is then passed to the vector search for better retrieval of relevant documents.

- **Contextualization**: Use the LLM to create a contextual answer (even if speculative) and pass this along with the original query to improve search results by adding more relevant context.

## Query Expansion with Multiple Queries:

- **Generate Multiple Related Queries**: Instead of relying on a single query, pass multiple related queries (generated by an LLM) alongside the original query to the RAG system. This broadens the search space and provides a wider pool of relevant documents.

- **Diversify Query Results**: This technique helps in retrieving more varied, yet relevant, results to improve the accuracy of the final answer.

## Decomposition for Complex Queries (Chain of Thought):

- **Break Down Complex Queries into Sub-queries**: For complex queries, decompose them into smaller, more manageable sub-queries. This method allows for sequential retrieval of answers to smaller questions, improving the accuracy and focus of the final response.

- **Example**: Complex questions can be broken down into multiple simpler ones and processed step-by-step.

## Retrieval Windowing:

- **Extend Context Window for LLMs**: When performing vector searches, retrieve not only the closest matches but also adjacent, contextually relevant chunks of text. This helps in providing a more complete and grounded answer by combining related content that may not appear in the same vector search result.

## Query Validation:

- **Ensure Relevance of Retrieved Results**: After performing a vector search, prompt the LLM to validate whether the retrieved results are truly relevant to the original query. If the results are irrelevant, the LLM should indicate that no relevant answer was found.

## Re-ranking with Cross-encoders:

- **Re-rank Retrieved Results by Relevance**: After increasing the set of results (e.g., through multiple queries), use a cross-encoder to assign relevance scores to each retrieved document. The cross-encoder compares the query and the retrieved documents in a detailed manner to provide a score and re-rank them.

- **Narrow Down Results**: The top-ranked results are sent to the LLM, ensuring only the most relevant information is used to generate an answer.

## Embedding Adapters:

- **Adapt Query Embeddings Using Feedback**: Use user feedback to adjust query embeddings. This approach tailors the query embedding by moving it closer to where the relevant answers are located, improving the relevance of the results retrieved.

- **Training an Embedding Adapter**: Adapt and refine the embedding space using labeled data (relevant vs. irrelevant results) to create an embedding adapter that will enhance the retrieval process.

## Supervised Tuning of Embedding Models:

- **Fine-tuning Embedding Models**: Perform supervised fine-tuning on the embedding model to improve its performance on specific types of queries. This is done by providing labeled training data (queries, answers, and relevance scores) and fine-tuning t
